{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLcnMM1tz9Q9u8SfvM0cAJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## MNIST via CNN"],"metadata":{"id":"ZvwOp4vbeiVc"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GuJRtPvFOqMg","executionInfo":{"status":"ok","timestamp":1686101253181,"user_tz":240,"elapsed":17403,"user":{"displayName":"Lindong Liu","userId":"17701052179384179066"}},"outputId":"fb8d3cc1-eafc-49d6-f1c1-564d794c56a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","\n","# Define the convolutional neural network (CNN)\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5)  # chnl-in, out, krnl\n","        self.conv2 = nn.Conv2d(32, 64, 5)\n","        self.fc1 = nn.Linear(1024, 512)   # [64*4*4, x]\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, 10)     # 10 classes\n","        self.pool1 = nn.MaxPool2d(2, stride=2)\n","        self.pool2 = nn.MaxPool2d(2, stride=2)\n","        self.drop1 = nn.Dropout(0.25)\n","        self.drop2 = nn.Dropout(0.50)\n","\n","    def forward(self, x):\n","        # convolution phase\n","        z = F.relu(self.conv1(x))   # Size([bs, 32, 24, 24])\n","        z = self.pool1(z)           # Size([bs, 32, 12, 12])\n","        z = self.drop1(z)\n","        z = F.relu(self.conv2(z))   # Size([bs, 64, 8, 8])\n","        z = self.pool2(z)           # Size([bs, 64, 4, 4])\n","\n","        # neural network phase\n","        z = z.reshape(-1, 1024)     # Size([bs, 1024])\n","        z = F.relu(self.fc1(z))     # Size([bs, 512])\n","        z = self.drop2(z)\n","        z = F.relu(self.fc2(z))     # Size([bs, 256])\n","        z = self.fc3(z)             # Size([bs, 10])\n","        return z\n","\n","# Load MNIST dataset\n","batch_size = 64\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize network and optimizer\n","model = Net()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# Training loop\n","for epoch in range(10):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Forward pass\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLc_ByJ-Tutv","executionInfo":{"status":"ok","timestamp":1686065603935,"user_tz":240,"elapsed":552269,"user":{"displayName":"Lindong Liu","userId":"17701052179384179066"}},"outputId":"768daae6-31e2-4c32-d8a0-2b2692decca7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 199954364.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 76329989.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 55367438.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 6096169.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch 1, Loss: 0.3991948068141937\n","Epoch 2, Loss: 0.46641403436660767\n","Epoch 3, Loss: 0.19204159080982208\n","Epoch 4, Loss: 0.15419679880142212\n","Epoch 5, Loss: 0.24739806354045868\n","Epoch 6, Loss: 0.17561829090118408\n","Epoch 7, Loss: 0.03065049648284912\n","Epoch 8, Loss: 0.10982850939035416\n","Epoch 9, Loss: 0.10394237190485\n","Epoch 10, Loss: 0.023665310814976692\n"]}]},{"cell_type":"markdown","source":["## SGHMC algorithm"],"metadata":{"id":"aQ640nB7eUyq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","class SGHMC(torch.optim.Optimizer):\n","    def __init__(self, params, lr=1e-2, scale_grad=1., gamma=1., beta=1.):\n","        defaults = dict(lr=lr, scale_grad=scale_grad, gamma=gamma, beta=beta)\n","        super(SGHMC, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['previous_grad'] = torch.zeros_like(p.data)\n","                    state['velocity'] = torch.zeros_like(p.data)\n","                \n","                previous_grad = state['previous_grad']\n","                velocity = state['velocity']\n","\n","                # Noise term\n","                noise = torch.normal(mean=0., std=torch.sqrt(torch.tensor(2.*group['gamma']*group['beta'] - 1))).to(p.data.device)\n","\n","                # Update the velocity and the previous gradient\n","                velocity = velocity * -group['gamma'] + d_p + noise\n","                previous_grad = d_p.clone()\n","\n","                # Update the parameters\n","                p.data.add_(-group['lr'], velocity)\n","\n","\n","# Define the convolutional neural network (CNN)\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5)  # chnl-in, out, krnl\n","        self.conv2 = nn.Conv2d(32, 64, 5)\n","        self.fc1 = nn.Linear(1024, 512)   # [64*4*4, x]\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, 10)     # 10 classes\n","        self.pool1 = nn.MaxPool2d(2, stride=2)\n","        self.pool2 = nn.MaxPool2d(2, stride=2)\n","        self.drop1 = nn.Dropout(0.25)\n","        self.drop2 = nn.Dropout(0.50)\n","\n","    def forward(self, x):\n","        # convolution phase\n","        z = F.relu(self.conv1(x))   # Size([bs, 32, 24, 24])\n","        z = self.pool1(z)           # Size([bs, 32, 12, 12])\n","        z = self.drop1(z)\n","        z = F.relu(self.conv2(z))   # Size([bs, 64, 8, 8])\n","        z = self.pool2(z)           # Size([bs, 64, 4, 4])\n","\n","        # neural network phase\n","        z = z.reshape(-1, 1024)     # Size([bs, 1024])\n","        z = F.relu(self.fc1(z))     # Size([bs, 512])\n","        z = self.drop2(z)\n","        z = F.relu(self.fc2(z))     # Size([bs, 256])\n","        z = self.fc3(z)             # Size([bs, 10])\n","        return z\n","\n","# Load MNIST dataset\n","batch_size = 64\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize network and optimizer\n","model = Net()\n","optimizer = SGHMC(model.parameters(), lr=0.01)\n","\n","# Training loop\n","for epoch in range(10):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        # Forward pass\n","        output = model(data)\n","        loss = F.cross_entropy(output, target)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSPe6GGFajoR","executionInfo":{"status":"ok","timestamp":1686097088994,"user_tz":240,"elapsed":741433,"user":{"displayName":"Lindong Liu","userId":"17701052179384179066"}},"outputId":"bb4ad723-8607-45a4-fcc5-1f30b1c71453"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 2.3077194690704346\n","Epoch 2, Loss: 2.298581600189209\n","Epoch 3, Loss: 2.3072409629821777\n","Epoch 4, Loss: 2.2922768592834473\n","Epoch 5, Loss: 2.533958673477173\n","Epoch 6, Loss: 2.307716131210327\n","Epoch 7, Loss: 2.2900078296661377\n","Epoch 8, Loss: 2.302164316177368\n","Epoch 9, Loss: 2.301670789718628\n","Epoch 10, Loss: 2.3042824268341064\n"]}]},{"cell_type":"markdown","source":["## Compare with SGD and Adam"],"metadata":{"id":"AkPk8Sjgd8_4"}},{"cell_type":"code","source":["# List of optimizers to compare\n","optimizers = [\"SGD\", \"Adam\", \"SGHMC\"]\n","\n","# Store the results\n","results = {}\n","\n","# Training loop\n","for opt_name in optimizers:\n","    # Initialize network for each optimizer\n","    model = Net()\n","\n","    if opt_name == \"SGD\":\n","        optimizer = optim.SGD(model.parameters(), lr=0.01)\n","    elif opt_name == \"Adam\":\n","        optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    elif opt_name == \"SGHMC\":\n","        optimizer = SGHMC(model.parameters(), lr=0.01)\n","\n","    print(f\"\\nTraining with {opt_name} optimizer:\")\n","    for epoch in range(10):\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            # Forward pass\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n","\n","    # Evaluation\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    results[opt_name] = accuracy\n","    print(f'Accuracy of the network on the test images using {opt_name}: {accuracy}%')\n","\n","# Print the final results\n","print(\"\\nFinal Results:\")\n","for opt_name, accuracy in results.items():\n","    print(f'{opt_name} Accuracy: {accuracy}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQIVWcoda9AI","executionInfo":{"status":"ok","timestamp":1686099583041,"user_tz":240,"elapsed":2303329,"user":{"displayName":"Lindong Liu","userId":"17701052179384179066"}},"outputId":"46b4cef7-e3a8-4f0b-a83c-56c5c2b4467a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with SGD optimizer:\n","Epoch 1, Loss: 0.5428701639175415\n","Epoch 2, Loss: 0.33463892340660095\n","Epoch 3, Loss: 0.2645123302936554\n","Epoch 4, Loss: 0.23696470260620117\n","Epoch 5, Loss: 0.2525932192802429\n","Epoch 6, Loss: 0.19202767312526703\n","Epoch 7, Loss: 0.10083306580781937\n","Epoch 8, Loss: 0.19862957298755646\n","Epoch 9, Loss: 0.21516868472099304\n","Epoch 10, Loss: 0.043656159192323685\n","Accuracy of the network on the test images using SGD: 97.67%\n","\n","Training with Adam optimizer:\n","Epoch 1, Loss: 0.2932542860507965\n","Epoch 2, Loss: 0.24368619918823242\n","Epoch 3, Loss: 0.336516797542572\n","Epoch 4, Loss: 0.017983315512537956\n","Epoch 5, Loss: 0.1276865154504776\n","Epoch 6, Loss: 0.024428585544228554\n","Epoch 7, Loss: 0.19916220009326935\n","Epoch 8, Loss: 0.18910767138004303\n","Epoch 9, Loss: 0.34055426716804504\n","Epoch 10, Loss: 0.019781511276960373\n","Accuracy of the network on the test images using Adam: 96.54%\n","\n","Training with SGHMC optimizer:\n","Epoch 1, Loss: 2.315648317337036\n","Epoch 2, Loss: 2.296440362930298\n","Epoch 3, Loss: 2.323167562484741\n","Epoch 4, Loss: 2.3049464225769043\n","Epoch 5, Loss: 2.2838892936706543\n","Epoch 6, Loss: 2.305032968521118\n","Epoch 7, Loss: 2.293958902359009\n","Epoch 8, Loss: 2.299492597579956\n","Epoch 9, Loss: 2.468212127685547\n","Epoch 10, Loss: 2.325566053390503\n","Accuracy of the network on the test images using SGHMC: 11.35%\n","\n","Final Results:\n","SGD Accuracy: 97.67%\n","Adam Accuracy: 96.54%\n","SGHMC Accuracy: 11.35%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zEvOWV5kmIPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gdUmHFAYmIOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training SGHMC"],"metadata":{"id":"-CP_UDlGeFae"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5)  # chnl-in, out, krnl\n","        self.conv2 = nn.Conv2d(32, 64, 5)\n","        self.fc1 = nn.Linear(1024, 512)   # [64*4*4, x]\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, 10)     # 10 classes\n","        self.pool1 = nn.MaxPool2d(2, stride=2)\n","        self.pool2 = nn.MaxPool2d(2, stride=2)\n","        self.drop1 = nn.Dropout(0.25)\n","        self.drop2 = nn.Dropout(0.50)\n","\n","    def forward(self, x):\n","        # convolution phase\n","        z = F.relu(self.conv1(x))   # Size([bs, 32, 24, 24])\n","        z = self.pool1(z)           # Size([bs, 32, 12, 12])\n","        z = self.drop1(z)\n","        z = F.relu(self.conv2(z))   # Size([bs, 64, 8, 8])\n","        z = self.pool2(z)           # Size([bs, 64, 4, 4])\n","\n","        # neural network phase\n","        z = z.reshape(-1, 1024)     # Size([bs, 1024])\n","        z = F.relu(self.fc1(z))     # Size([bs, 512])\n","        z = self.drop2(z)\n","        z = F.relu(self.fc2(z))     # Size([bs, 256])\n","        z = self.fc3(z)             # Size([bs, 10])\n","        return z\n","\n","class SGHMC(torch.optim.Optimizer):\n","    def __init__(self, params, lr=1e-2, scale_grad=1., gamma=1., beta=1.):\n","        defaults = dict(lr=lr, scale_grad=scale_grad, gamma=gamma, beta=beta)\n","        super(SGHMC, self).__init__(params, defaults)\n","\n","    def step(self, closure=None):\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['previous_grad'] = torch.zeros_like(p.data)\n","                    state['velocity'] = torch.zeros_like(p.data)\n","                \n","                previous_grad = state['previous_grad']\n","                velocity = state['velocity']\n","\n","                # Noise term\n","                noise = torch.normal(mean=0., std=torch.sqrt(torch.tensor(2.*group['gamma']*group['beta'] - 1))).to(p.data.device)\n","\n","                # Update the velocity and the previous gradient\n","                velocity = velocity * -group['gamma'] + d_p + noise\n","                previous_grad = d_p.clone()\n","\n","                # Update the parameters\n","                p.data.add_(-group['lr'], velocity)\n","\n","# Load MNIST dataset\n","batch_size = 64\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# List of optimizers to compare\n","optimizers = [\"SGHMC_gamma1\"]\n","\n","# Store the results\n","results = {}\n","\n","# Training loop\n","for opt_name in optimizers:\n","    # Initialize network for each optimizer\n","    model = Net()\n","\n","    if opt_name == \"SGHMC_gamma1\":\n","        optimizer = SGHMC(model.parameters(), lr=0.01, gamma=20.)\n","    # elif opt_name == \"SGHMC_gamma2\":\n","    #     optimizer = SGHMC(model.parameters(), lr=0.01, gamma=2.)\n","    # elif opt_name == \"SGHMC_gamma3\":\n","    #     optimizer = SGHMC(model.parameters(), lr=0.01, gamma=3.)\n","    # elif opt_name == \"SGHMC_beta1\":\n","    #     optimizer = SGHMC(model.parameters(), lr=0.01, beta=1.)\n","    # elif opt_name == \"SGHMC_beta2\":\n","    #     optimizer = SGHMC(model.parameters(), lr=0.01, beta=2.)\n","    # elif opt_name == \"SGHMC_beta3\":\n","    #     optimizer = SGHMC(model.parameters(), lr=0.01, beta=3.)\n","\n","    print(f\"\\nTraining with {opt_name} optimizer:\")\n","    for epoch in range(20):\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            # Forward pass\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n","\n","    # Evaluation\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += target.size(0)\n","            correct += (predicted == target).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    results[opt_name] = accuracy\n","    print(f'Accuracy of the network on the test images using {opt_name}: {accuracy}%')\n","\n","# Print the final results\n","print(\"\\nFinal Results:\")\n","for opt_name, accuracy in results.items():\n","    print(f'{opt_name} Accuracy: {accuracy}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PB6ia2o4bAMT","executionInfo":{"status":"ok","timestamp":1686155246801,"user_tz":240,"elapsed":1351029,"user":{"displayName":"Lindong Liu","userId":"17701052179384179066"}},"outputId":"4e6caee2-c32b-49ff-9a56-305abb74e8f9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 131256763.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 86095020.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 34860959.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 15901943.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","\n","Training with SGHMC_gamma1 optimizer:\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-6152f1f198b0>:65: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  p.data.add_(-group['lr'], velocity)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 2.304826498031616\n","Epoch 2, Loss: 2.2983460426330566\n","Epoch 3, Loss: 2.303752899169922\n","Epoch 4, Loss: 2.3017473220825195\n","Epoch 5, Loss: 2.289079189300537\n","Epoch 6, Loss: 2.305964708328247\n","Epoch 7, Loss: 2.3190977573394775\n","Epoch 8, Loss: 2.3076488971710205\n","Epoch 9, Loss: 2.28902006149292\n","Epoch 10, Loss: 2.2949905395507812\n","Epoch 11, Loss: 2.2826132774353027\n","Epoch 12, Loss: 2.307840347290039\n","Epoch 13, Loss: 2.3168587684631348\n","Epoch 14, Loss: 2.2804901599884033\n","Epoch 15, Loss: 2.2970571517944336\n","Epoch 16, Loss: 2.280909299850464\n","Epoch 17, Loss: 2.3005664348602295\n","Epoch 18, Loss: 2.3042876720428467\n","Epoch 19, Loss: 2.3109662532806396\n","Epoch 20, Loss: 2.2855658531188965\n","Accuracy of the network on the test images using SGHMC_gamma1: 11.35%\n","\n","Final Results:\n","SGHMC_gamma1 Accuracy: 11.35%\n"]}]}]}